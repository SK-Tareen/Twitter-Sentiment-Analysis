{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "253fef3e",
      "metadata": {
        "id": "253fef3e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuvDtzznAwgs",
        "outputId": "67f6d678-a438-4107-bdb6-dc40f0b4e976"
      },
      "id": "fuvDtzznAwgs",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "298f3545",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "298f3545",
        "outputId": "7d3d54b6-04d1-4727-8561-417c2cbcc991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       textID                                               text  \\\n",
            "0  cb774db0d1                I`d have responded, if I were going   \n",
            "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
            "2  088c60f138                          my boss is bullying me...   \n",
            "3  9642c003ef                     what interview! leave me alone   \n",
            "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
            "\n",
            "                         selected_text sentiment Time of Tweet Age of User  \\\n",
            "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
            "1                             Sooo SAD  negative          noon       21-30   \n",
            "2                          bullying me  negative         night       31-45   \n",
            "3                       leave me alone  negative       morning       46-60   \n",
            "4                        Sons of ****,  negative          noon       60-70   \n",
            "\n",
            "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \\\n",
            "0  Afghanistan          38928346         652860.0               60   \n",
            "1      Albania           2877797          27400.0              105   \n",
            "2      Algeria          43851044        2381740.0               18   \n",
            "3      Andorra             77265            470.0              164   \n",
            "4       Angola          32866272        1246700.0               26   \n",
            "\n",
            "                           cleaned_text  \\\n",
            "0                       responded going   \n",
            "1               soon sad miss san diego   \n",
            "2                          boy building   \n",
            "3                 interview leave alone   \n",
            "4  son could put release already bought   \n",
            "\n",
            "                                              tokens  \n",
            "0                             ['responded', 'going']  \n",
            "1            ['sooo', 'sad', 'miss', 'san', 'diego']  \n",
            "2                                ['bos', 'bullying']  \n",
            "3                    ['interview', 'leave', 'alone']  \n",
            "4  ['son', 'could', 'put', 'release', 'already', ...  \n"
          ]
        }
      ],
      "source": [
        "trainData = pd.read_csv('/content/drive/MyDrive/finalDataset.csv')\n",
        "trainData = trainData.dropna()\n",
        "print(trainData.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "691e1b59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "691e1b59",
        "outputId": "d1e333d7-960b-4acd-e624-11454d502e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                          responded going\n",
            "1                                  soon sad miss san diego\n",
            "2                                             boy building\n",
            "3                                    interview leave alone\n",
            "4                     son could put release already bought\n",
            "                               ...                        \n",
            "27475    wish could come see denver husband lost job af...\n",
            "27476    wondered rake client made clear net force des ...\n",
            "27477    may good enjoy break probably need hectic week...\n",
            "27478                                                worth\n",
            "27479                      flirting going at smile may hug\n",
            "Name: cleaned_text, Length: 27368, dtype: object\n"
          ]
        }
      ],
      "source": [
        "clean_tweets = trainData['cleaned_text']\n",
        "print(clean_tweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "827c4283",
      "metadata": {
        "id": "827c4283"
      },
      "source": [
        "ngram_range = (1,2) --> since it considers unigrams (aka individual words) and two-word combinations (bigrams) too so it takes better context such as 'machine learning' or 'data science'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9be2e561",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9be2e561",
        "outputId": "9d179f41-3c2d-442d-dc80-5d8c4494717d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27368, 132139)\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(max_features=None, ngram_range = (1,2))\n",
        "train_data_features = vectorizer.fit_transform(clean_tweets)\n",
        "print(train_data_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e8499be3",
      "metadata": {
        "id": "e8499be3"
      },
      "outputs": [],
      "source": [
        "y_train = trainData['sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3fac0a3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "3fac0a3c",
        "outputId": "cce8770f-99c0-463a-bc01-2a8bb7ad0708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(multi_class='multinomial', random_state=0)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;, random_state=0)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
        "model.fit(train_data_features, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7000ca75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7000ca75",
        "outputId": "1df9bed1-399f-470e-d172-0d721867be80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       textID                                               text sentiment  \\\n",
            "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
            "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive   \n",
            "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative   \n",
            "3  01082688c6                                        happy bday!  positive   \n",
            "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive   \n",
            "\n",
            "  Time of Tweet Age of User      Country  Population -2020  Land Area (Km²)  \\\n",
            "0       morning        0-20  Afghanistan        38928346.0         652860.0   \n",
            "1          noon       21-30      Albania         2877797.0          27400.0   \n",
            "2         night       31-45      Algeria        43851044.0        2381740.0   \n",
            "3       morning       46-60      Andorra           77265.0            470.0   \n",
            "4          noon       60-70       Angola        32866272.0        1246700.0   \n",
            "\n",
            "   Density (P/Km²)                                       cleaned_text  \\\n",
            "0             60.0                                   last session day   \n",
            "1            105.0  shanghai also really exciting precisely skyscr...   \n",
            "2             18.0  recession hit veronique branquinho quit compan...   \n",
            "3            164.0                                     happy birthday   \n",
            "4             26.0                                               like   \n",
            "\n",
            "                                              tokens  \n",
            "0                         ['last', 'session', 'day']  \n",
            "1  ['shanghai', 'also', 'really', 'exciting', 'pr...  \n",
            "2  ['recession', 'hit', 'veronique', 'branquinho'...  \n",
            "3                              ['happy', 'birthday']  \n",
            "4                                           ['like']  \n"
          ]
        }
      ],
      "source": [
        "testData = pd.read_csv('/content/drive/MyDrive/finalDatasetTest.csv')\n",
        "testData = testData.dropna()\n",
        "print(testData.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "63298105",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63298105",
        "outputId": "3b231b91-8e0a-4e4d-b729-4b38f237198f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                        last session day\n",
            "1       shanghai also really exciting precisely skyscr...\n",
            "2       recession hit veronique branquinho quit compan...\n",
            "3                                          happy birthday\n",
            "4                                                    like\n",
            "                              ...                        \n",
            "3529                                      tired sleep try\n",
            "3530    alone old house thanks net keep alive kicking ...\n",
            "3531    know mean little dog sinking depression want m...\n",
            "3532            supra next couture video going love video\n",
            "3533                                    omgssh and cut by\n",
            "Name: cleaned_text, Length: 3517, dtype: object\n"
          ]
        }
      ],
      "source": [
        "clean_tweets_test = testData['cleaned_text']\n",
        "print(clean_tweets_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "740c071f",
      "metadata": {
        "id": "740c071f"
      },
      "outputs": [],
      "source": [
        "y_test = testData['sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ef371823",
      "metadata": {
        "id": "ef371823"
      },
      "outputs": [],
      "source": [
        "test_data_features = vectorizer.transform(clean_tweets_test)\n",
        "result = model.predict(test_data_features)\n",
        "output = pd.DataFrame( data={\"id\":testData[\"textID\"], \"sentiment\":result} )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "926ca5c2",
      "metadata": {
        "id": "926ca5c2"
      },
      "outputs": [],
      "source": [
        "output.to_csv(\"resultPredictionsOnTest.csv\", index=False, quoting=3 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6544fadf",
      "metadata": {
        "id": "6544fadf",
        "outputId": "6eaf8791-16e9-4ec2-bb79-6c5b7972af87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6960\n",
            "Precision: 0.7006\n",
            "Recall: 0.6960\n",
            "F1 Score: 0.6962\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.62      0.66       999\n",
            "     neutral       0.64      0.72      0.68      1415\n",
            "    positive       0.76      0.74      0.75      1103\n",
            "\n",
            "    accuracy                           0.70      3517\n",
            "   macro avg       0.71      0.69      0.70      3517\n",
            "weighted avg       0.70      0.70      0.70      3517\n",
            "\n"
          ]
        }
      ],
      "source": [
        "accuracy = accuracy_score(y_test, result)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "precision = precision_score(y_test, result, average='weighted', zero_division=0) # weighted since is multiclass classification\n",
        "recall = recall_score(y_test, result, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, result, average='weighted', zero_division=0)\n",
        "\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "# Print a classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, result))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "452b30c3",
      "metadata": {
        "id": "452b30c3"
      },
      "source": [
        "NEG SPACY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "492ef319",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "492ef319",
        "outputId": "e0d6e133-693d-4af9-99c2-5b35d88bf2c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting negspacy\n",
            "  Downloading negspacy-1.0.4.tar.gz (13 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from negspacy) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.1->negspacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.1->negspacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.1->negspacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.1->negspacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.1->negspacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.1->negspacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.1->negspacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.1->negspacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.1->negspacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.1->negspacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.1->negspacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.1->negspacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.1->negspacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.1->negspacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.1->negspacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.1->negspacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.0.1->negspacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.1->negspacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.1->negspacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.1->negspacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.1->negspacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.1->negspacy) (0.1.2)\n",
            "Building wheels for collected packages: negspacy\n",
            "  Building wheel for negspacy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for negspacy: filename=negspacy-1.0.4-py3-none-any.whl size=12537 sha256=7ef3a88a3c3f25b30eb0aaf825719028cc76d43006189d6841cc187863e573aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/31/f0/3a217aaedf320e4df653347cd6538f3648263b864c8e140853\n",
            "Successfully built negspacy\n",
            "Installing collected packages: negspacy\n",
            "Successfully installed negspacy-1.0.4\n"
          ]
        }
      ],
      "source": [
        "pip install negspacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf9abab9",
      "metadata": {
        "id": "bf9abab9",
        "outputId": "a67078e1-e46d-439c-f36b-56703f143d85"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'en_core_web_sm'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01men_core_web_sm\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m en_core_web_sm\u001b[38;5;241m.\u001b[39mload()\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'en_core_web_sm'"
          ]
        }
      ],
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import spacy\n",
        "from spacy.tokens import Token\n",
        "from negspacy.negation import Negex\n",
        "from negspacy.termsets import termset\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Set up the termset and get patterns\n",
        "ts = termset(\"en\")\n",
        "\n",
        "# Register the 'negex' extension on tokens to avoid any missing extension error\n",
        "Token.set_extension(\"negex\", default=False)\n",
        "\n",
        "# Add Negex component with configuration using termset patterns\n",
        "nlp.add_pipe(\"negex\", last=True, config={\n",
        "    \"neg_termset\": ts.get_patterns(),\n",
        "    \"ent_types\": [\"PERSON\", \"NOUN\"],  # Relevant entity types for negation analysis\n",
        "    \"chunk_prefix\": [\"no\", \"not\", \"never\", \"without\"]  # Define common negation indicators\n",
        "})\n",
        "\n",
        "# Load training data\n",
        "trainData = pd.read_csv('/content/drive/MyDrive/finalDataset.csv')\n",
        "trainData = trainData.dropna()\n",
        "print(trainData.head())\n",
        "\n",
        "# Clean tweets\n",
        "clean_tweets = trainData['cleaned_text']\n",
        "print(clean_tweets)\n",
        "\n",
        "# Analyze negation with Negex\n",
        "def analyze_negation(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.text + \"NEG\" if token._.negex else token.text for token in doc])\n",
        "\n",
        "# Apply Negex analysis to cleaned tweets\n",
        "clean_tweets = clean_tweets.apply(analyze_negation)\n",
        "\n",
        "# Vectorize tweets\n",
        "vectorizer = TfidfVectorizer(max_features=None, ngram_range=(1, 2))\n",
        "train_data_features = vectorizer.fit_transform(clean_tweets)\n",
        "print(train_data_features.shape)\n",
        "\n",
        "# Get labels\n",
        "y_train = trainData['sentiment']\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
        "model.fit(train_data_features, y_train)\n",
        "\n",
        "# Load test data\n",
        "testData = pd.read_csv('/content/drive/MyDrive/finalDatasetTest.csv')\n",
        "testData = testData.dropna()\n",
        "print(testData.head())\n",
        "\n",
        "# Clean test tweets\n",
        "clean_tweets_test = testData['cleaned_text']\n",
        "print(clean_tweets_test)\n",
        "\n",
        "# Analyze negation on test data\n",
        "clean_tweets_test = clean_tweets_test.apply(analyze_negation)\n",
        "\n",
        "# Get true labels\n",
        "y_test = testData['sentiment']\n",
        "\n",
        "# Vectorize test data\n",
        "test_data_features = vectorizer.transform(clean_tweets_test)\n",
        "result = model.predict(test_data_features)\n",
        "\n",
        "# Create output DataFrame\n",
        "output = pd.DataFrame(data={\"id\": testData[\"textID\"], \"sentiment\": result})\n",
        "\n",
        "# Save predictions to CSV\n",
        "output.to_csv(\"resultPredictionsOnTest.csv\", index=False, quoting=3)\n",
        "\n",
        "# Calculate and print metrics\n",
        "accuracy = accuracy_score(y_test, result)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "precision = precision_score(y_test, result, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, result, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, result, average='weighted', zero_division=0)\n",
        "\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "# Print a classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, result))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1boiolNmD9YK",
        "outputId": "f4a2faf4-a4ec-4864-f76a-19752c2d773b"
      },
      "id": "1boiolNmD9YK",
      "execution_count": 26,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       textID                                               text  \\\n",
            "0  cb774db0d1                I`d have responded, if I were going   \n",
            "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
            "2  088c60f138                          my boss is bullying me...   \n",
            "3  9642c003ef                     what interview! leave me alone   \n",
            "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
            "\n",
            "                         selected_text sentiment Time of Tweet Age of User  \\\n",
            "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
            "1                             Sooo SAD  negative          noon       21-30   \n",
            "2                          bullying me  negative         night       31-45   \n",
            "3                       leave me alone  negative       morning       46-60   \n",
            "4                        Sons of ****,  negative          noon       60-70   \n",
            "\n",
            "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \\\n",
            "0  Afghanistan          38928346         652860.0               60   \n",
            "1      Albania           2877797          27400.0              105   \n",
            "2      Algeria          43851044        2381740.0               18   \n",
            "3      Andorra             77265            470.0              164   \n",
            "4       Angola          32866272        1246700.0               26   \n",
            "\n",
            "                           cleaned_text  \\\n",
            "0                       responded going   \n",
            "1               soon sad miss san diego   \n",
            "2                          boy building   \n",
            "3                 interview leave alone   \n",
            "4  son could put release already bought   \n",
            "\n",
            "                                              tokens  \n",
            "0                             ['responded', 'going']  \n",
            "1            ['sooo', 'sad', 'miss', 'san', 'diego']  \n",
            "2                                ['bos', 'bullying']  \n",
            "3                    ['interview', 'leave', 'alone']  \n",
            "4  ['son', 'could', 'put', 'release', 'already', ...  \n",
            "0                                          responded going\n",
            "1                                  soon sad miss san diego\n",
            "2                                             boy building\n",
            "3                                    interview leave alone\n",
            "4                     son could put release already bought\n",
            "                               ...                        \n",
            "27475    wish could come see denver husband lost job af...\n",
            "27476    wondered rake client made clear net force des ...\n",
            "27477    may good enjoy break probably need hectic week...\n",
            "27478                                                worth\n",
            "27479                      flirting going at smile may hug\n",
            "Name: cleaned_text, Length: 27368, dtype: object\n",
            "(27368, 132138)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       textID                                               text sentiment  \\\n",
            "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
            "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive   \n",
            "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative   \n",
            "3  01082688c6                                        happy bday!  positive   \n",
            "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive   \n",
            "\n",
            "  Time of Tweet Age of User      Country  Population -2020  Land Area (Km²)  \\\n",
            "0       morning        0-20  Afghanistan        38928346.0         652860.0   \n",
            "1          noon       21-30      Albania         2877797.0          27400.0   \n",
            "2         night       31-45      Algeria        43851044.0        2381740.0   \n",
            "3       morning       46-60      Andorra           77265.0            470.0   \n",
            "4          noon       60-70       Angola        32866272.0        1246700.0   \n",
            "\n",
            "   Density (P/Km²)                                       cleaned_text  \\\n",
            "0             60.0                                   last session day   \n",
            "1            105.0  shanghai also really exciting precisely skyscr...   \n",
            "2             18.0  recession hit veronique branquinho quit compan...   \n",
            "3            164.0                                     happy birthday   \n",
            "4             26.0                                               like   \n",
            "\n",
            "                                              tokens  \n",
            "0                         ['last', 'session', 'day']  \n",
            "1  ['shanghai', 'also', 'really', 'exciting', 'pr...  \n",
            "2  ['recession', 'hit', 'veronique', 'branquinho'...  \n",
            "3                              ['happy', 'birthday']  \n",
            "4                                           ['like']  \n",
            "0                                        last session day\n",
            "1       shanghai also really exciting precisely skyscr...\n",
            "2       recession hit veronique branquinho quit compan...\n",
            "3                                          happy birthday\n",
            "4                                                    like\n",
            "                              ...                        \n",
            "3529                                      tired sleep try\n",
            "3530    alone old house thanks net keep alive kicking ...\n",
            "3531    know mean little dog sinking depression want m...\n",
            "3532            supra next couture video going love video\n",
            "3533                                    omgssh and cut by\n",
            "Name: cleaned_text, Length: 3517, dtype: object\n",
            "Accuracy: 0.6943\n",
            "Precision: 0.6993\n",
            "Recall: 0.6943\n",
            "F1 Score: 0.6945\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.61      0.66       999\n",
            "     neutral       0.64      0.72      0.68      1415\n",
            "    positive       0.76      0.73      0.75      1103\n",
            "\n",
            "    accuracy                           0.69      3517\n",
            "   macro avg       0.71      0.69      0.70      3517\n",
            "weighted avg       0.70      0.69      0.69      3517\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words = [\n",
        "    \"2-faced\", \"2-faces\", \"abnormal\", \"abolish\", \"abominable\", \"abominably\", \"abominate\",\n",
        "    \"abomination\", \"abort\", \"aborted\", \"aborts\", \"abrade\", \"abrasive\", \"abrupt\", \"abruptly\",\n",
        "    \"abscond\", \"absence\", \"absent-minded\", \"absentee\", \"absurd\", \"absurdity\", \"absurdly\",\n",
        "    \"absurdness\", \"abuse\", \"abused\", \"abuses\", \"abusive\", \"abysmal\", \"abysmally\", \"abyss\",\n",
        "    \"accidental\", \"accost\", \"accursed\", \"accusation\", \"accusations\", \"accuse\", \"accuses\",\n",
        "    \"accusing\", \"accusingly\", \"acerbate\", \"acerbic\", \"acerbically\", \"ache\", \"ached\", \"aches\",\n",
        "    \"achey\", \"aching\", \"acrid\", \"acridly\", \"acridness\", \"acrimonious\", \"acrimoniously\",\n",
        "    \"acrimony\", \"adamant\", \"adamantly\", \"addict\", \"addicted\", \"addicting\", \"addicts\",\n",
        "    \"admonish\", \"admonisher\", \"admonishingly\", \"admonishment\", \"admonition\", \"adulterate\",\n",
        "    \"adulterated\", \"adulteration\", \"adulterier\", \"adversarial\", \"adversary\", \"adverse\",\n",
        "    \"adversity\", \"afflict\", \"affliction\", \"afflictive\", \"affront\", \"afraid\", \"aggravate\",\n",
        "    \"aggravating\", \"aggravation\", \"aggression\", \"aggressive\", \"aggressiveness\", \"aggressor\",\n",
        "    \"aggrieve\", \"aggrieved\", \"aggrivation\", \"aghast\", \"agonies\", \"agonize\", \"agonizing\",\n",
        "    \"agonizingly\", \"agony\", \"aground\", \"ail\", \"ailing\", \"ailment\", \"aimless\", \"alarm\",\n",
        "    \"alarmed\", \"alarming\", \"alarmingly\", \"alienate\", \"alienated\", \"alienation\", \"allegation\",\n",
        "    \"allegations\", \"allege\", \"allergic\", \"allergies\", \"allergy\", \"aloof\", \"altercation\",\n",
        "    \"ambiguity\", \"ambiguous\", \"ambivalence\", \"ambivalent\", \"ambush\", \"amiss\", \"amputate\",\n",
        "    \"anarchism\", \"anarchist\", \"anarchistic\", \"anarchy\", \"anemic\", \"anger\", \"angrily\",\n",
        "    \"angriness\", \"angry\", \"anguish\", \"animosity\", \"annihilate\", \"annihilation\", \"annoy\",\n",
        "    \"annoyance\", \"annoyances\", \"annoyed\", \"annoying\", \"annoyingly\", \"annoys\", \"anomalous\",\n",
        "    \"anomaly\", \"antagonism\", \"antagonist\", \"antagonistic\", \"antagonize\", \"anti-\", \"anti-american\",\n",
        "    \"anti-israeli\", \"anti-occupation\", \"anti-proliferation\", \"anti-semites\", \"anti-social\",\n",
        "    \"anti-us\", \"anti-white\", \"antipathy\", \"antiquated\", \"antithetical\", \"anxieties\", \"anxiety\",\n",
        "    \"anxious\", \"anxiously\", \"anxiousness\", \"apathetic\", \"apathetically\", \"apathy\", \"apocalypse\",\n",
        "    \"apocalyptic\", \"apologist\", \"apologists\", \"appal\", \"appall\", \"appalled\", \"appalling\",\n",
        "    \"appallingly\", \"apprehension\", \"apprehensions\", \"apprehensive\", \"apprehensively\", \"arbitrary\",\n",
        "    \"arcane\", \"archaic\", \"arduous\", \"arduously\", \"argumentative\", \"arrogance\", \"arrogant\",\n",
        "    \"arrogantly\", \"ashamed\", \"asinine\", \"asininely\", \"asinininity\", \"askance\", \"asperse\",\n",
        "    \"aspersion\", \"aspersions\", \"assail\", \"assassin\", \"assassinate\", \"assault\", \"assult\",\n",
        "    \"astray\", \"asunder\", \"atrocious\", \"atrocities\", \"atrocity\", \"atrophy\", \"attack\", \"attacks\",\n",
        "    \"audacious\", \"audaciously\", \"audaciousness\", \"audacity\", \"audiciously\", \"austere\",\n",
        "    \"authoritarian\", \"autocrat\", \"autocratic\", \"avalanche\", \"avarice\", \"avaricious\",\n",
        "    \"avariciously\", \"avenge\", \"averse\", \"aversion\", \"aweful\", \"awful\", \"awfully\", \"awfulness\",\n",
        "    \"awkward\", \"awkwardness\", \"ax\", \"babble\", \"back-logged\", \"back-wood\", \"back-woods\",\n",
        "    \"backache\", \"backaches\", \"backaching\", \"backbite\", \"backbiting\", \"backward\", \"backwardness\",\n",
        "    \"backwood\", \"backwoods\", \"bad\", \"badly\", \"baffle\", \"baffled\", \"bafflement\", \"baffling\",\n",
        "    \"bait\", \"balk\", \"banal\", \"banalize\", \"bane\", \"banish\", \"banishment\", \"bankrupt\", \"barbarian\", \"barbaric\", \"barbarically\", \"barbarity\", \"barbarous\",\n",
        "    \"barbarously\", \"barren\", \"baseless\", \"bash\", \"bashed\", \"bashful\", \"bashing\", \"bastard\",\n",
        "    \"bastards\", \"battered\", \"battering\", \"batty\", \"bearish\", \"beastly\", \"bedlam\", \"bedlamite\",\n",
        "    \"befoul\", \"beg\", \"beggar\", \"beggarly\", \"begging\", \"beguile\", \"belabor\", \"belated\",\n",
        "    \"beleaguer\", \"belie\", \"belittle\", \"belittled\", \"belittling\", \"bellicose\", \"belligerence\",\n",
        "    \"belligerent\", \"belligerently\", \"bemoan\", \"bemoaning\", \"bemused\", \"bent\", \"berate\",\n",
        "    \"bereave\", \"bereavement\", \"bereft\", \"berserk\", \"beseech\", \"beset\", \"besiege\", \"besmirch\",\n",
        "    \"bestial\", \"betray\", \"betrayal\", \"betrayals\", \"betrayer\", \"betraying\", \"betrays\", \"bewail\",\n",
        "    \"beware\", \"bewilder\", \"bewildered\", \"bewildering\", \"bewilderingly\", \"bewilderment\", \"bewitch\",\n",
        "    \"bias\", \"biased\", \"biases\", \"bicker\", \"bickering\", \"bid-rigging\", \"bigotries\", \"bigotry\",\n",
        "    \"bitch\", \"bitchy\", \"biting\", \"bitingly\", \"bitter\", \"bitterly\", \"bitterness\", \"bizarre\",\n",
        "    \"blab\", \"blabber\", \"blackmail\", \"blah\", \"blame\", \"blameworthy\", \"bland\", \"blandish\",\n",
        "    \"blaspheme\", \"blasphemous\", \"blasphemy\", \"blasted\", \"blatant\", \"blatantly\", \"blather\", \"bleak\",\n",
        "    \"bleakly\", \"bleakness\", \"bleed\", \"bleeding\", \"bleeds\", \"blemish\", \"blind\", \"blinding\",\n",
        "    \"blindingly\", \"blindside\", \"blister\", \"blistering\", \"bloated\", \"blockage\", \"blockhead\",\n",
        "    \"bloodshed\", \"bloodthirsty\", \"bloody\", \"blotchy\", \"blow\", \"blunder\", \"blundering\", \"blunders\",\n",
        "    \"blunt\", \"blur\", \"bluring\", \"blurred\", \"blurring\", \"blurry\", \"blurs\", \"blurt\", \"boastful\",\n",
        "    \"boggle\", \"bogus\", \"boil\", \"boiling\", \"boisterous\", \"bomb\", \"bombard\", \"bombardment\",\n",
        "    \"bombastic\", \"bondage\", \"bonkers\", \"bore\", \"bored\", \"boredom\", \"bores\", \"boring\", \"botch\",\n",
        "    \"bother\", \"bothered\", \"bothering\", \"bothers\", \"bothersome\", \"bowdlerize\", \"boycott\",\n",
        "    \"braggart\", \"bragger\", \"brainless\", \"brainwash\", \"brash\", \"brashly\", \"brashness\", \"brat\",\n",
        "    \"bravado\", \"brazen\", \"brazenly\", \"brazenness\", \"breach\", \"break\", \"break-up\", \"break-ups\",\n",
        "    \"breakdown\", \"breaking\", \"breaks\", \"breakup\", \"breakups\", \"bribery\", \"brimstone\", \"bristle\",\n",
        "    \"brittle\", \"broke\", \"broken\", \"broken-hearted\", \"brood\", \"browbeat\", \"bruise\", \"bruised\",\n",
        "    \"bruises\", \"bruising\", \"brusque\", \"brutal\", \"brutalising\", \"brutalities\", \"brutality\",\n",
        "    \"brutalize\", \"brutalizing\", \"brutally\", \"brute\", \"brutish\", \"bs\", \"buckle\", \"bug\", \"bugging\",\n",
        "    \"buggy\", \"bugs\", \"bulkier\", \"bulkiness\", \"bulky\", \"bulkyness\", \"bull****\", \"bull----\",\n",
        "    \"bullies\", \"bullshit\", \"bullshyt\", \"bully\", \"bullying\", \"bullyingly\", \"bum\", \"bump\",\n",
        "    \"bumped\", \"bumping\", \"bumpping\", \"bumps\", \"bumpy\", \"bungle\", \"bungler\", \"bungling\", \"bunk\",\n",
        "    \"burden\", \"burdensome\", \"burdensomely\", \"burn\", \"burned\", \"burning\", \"burns\", \"bust\",\n",
        "    \"busts\", \"busybody\", \"butcher\", \"butchery\", \"buzzing\", \"byzantine\", \"cackle\", \"calamities\",\n",
        "    \"calamitous\", \"calamitously\", \"calamity\", \"callous\", \"calumniate\", \"calumniation\",\n",
        "    \"calumnies\", \"calumnious\", \"calumniously\", \"calumny\", \"cancer\", \"cancerous\", \"cannibal\",\n",
        "    \"cannibalize\", \"capitulate\", \"capricious\", \"capriciously\", \"capriciousness\", \"capsize\",\n",
        "    \"careless\", \"carelessness\", \"caricature\", \"carnage\", \"carp\", \"cartoonish\", \"cash-strapped\",\n",
        "    \"castigate\", \"castrated\", \"casualty\", \"cataclysm\", \"cataclysmal\", \"cataclysmic\",\n",
        "    \"cataclysmically\", \"catastrophe\", \"catastrophes\", \"catastrophic\", \"catastrophically\",\n",
        "    \"catastrophies\", \"caustic\", \"caustically\", \"cautionary\", \"cave\", \"censure\", \"chafe\",\n",
        "    \"chaff\", \"chagrin\", \"challenging\", \"chaos\", \"chaotic\", \"chasten\", \"chastise\", \"chastisement\",\n",
        "    \"chatter\", \"chatterbox\", \"cheap\", \"cheapen\", \"cheaply\", \"cheat\", \"cheated\", \"cheater\",\n",
        "    \"cheating\", \"cheats\", \"checkered\", \"cheerless\", \"cheesy\", \"chide\", \"childish\", \"chill\",\n",
        "    \"chilly\", \"chintzy\", \"choke\", \"choleric\", \"choppy\", \"chore\", \"chronic\", \"chunky\", \"clamor\",\n",
        "    \"clamorous\", \"clash\", \"cliche\", \"cliched\", \"clique\", \"clog\", \"clogged\", \"clogs\", \"cloud\",\n",
        "    \"clouding\", \"cloudy\", \"clueless\", \"clumsy\", \"clunky\", \"coarse\", \"cocky\", \"coerce\",\n",
        "    \"coercion\", \"coercive\", \"cold\", \"coldly\", \"collapse\", \"collude\", \"collusion\", \"combative\",\n",
        "    \"combust\", \"comical\", \"commiserate\", \"commonplace\", \"commotion\", \"commotions\",\n",
        "    \"complacent\", \"complain\", \"complained\", \"complaining\", \"complains\", \"complaint\",\n",
        "    \"complaints\", \"complex\", \"complicated\", \"complication\", \"complicit\", \"compulsion\",\n",
        "    \"compulsive\", \"concede\", \"conceded\", \"conceit\", \"conceited\", \"concen\", \"concens\",\n",
        "    \"concern\", \"concerned\", \"concerns\", \"concession\", \"concessions\", \"condemn\", \"condemnable\", \"condemnation\",\n",
        "    \"condemned\", \"condemns\", \"condescend\", \"condescending\", \"condescendingly\",\n",
        "    \"condescension\", \"confess\", \"confession\", \"confessions\", \"confined\", \"conflict\",\n",
        "    \"conflicted\", \"conflicting\", \"conflicts\", \"confound\", \"confounded\", \"confounding\",\n",
        "    \"confront\", \"confrontation\", \"confrontational\", \"confuse\", \"confused\", \"confuses\",\n",
        "    \"confusing\", \"confusion\", \"confusions\", \"congested\", \"congestion\", \"cons\",\n",
        "    \"conscons\", \"conservative\", \"conspicuous\", \"conspicuously\", \"conspiracies\",\n",
        "    \"conspiracy\", \"conspirator\", \"conspiratorial\", \"conspire\", \"consternation\",\n",
        "    \"contagious\", \"contaminate\", \"contaminated\", \"contaminates\", \"contaminating\",\n",
        "    \"contamination\", \"contempt\", \"contemptible\", \"contemptuous\", \"contemptuously\",\n",
        "    \"contend\", \"contention\", \"contentious\", \"contort\", \"contortions\", \"contradict\",\n",
        "    \"contradiction\", \"contradictory\", \"contrariness\", \"contravene\", \"contrive\",\n",
        "    \"contrived\", \"controversial\", \"controversy\", \"convoluted\", \"corrode\", \"corrosion\",\n",
        "    \"corrosions\", \"corrosive\", \"corrupt\", \"corrupted\", \"corrupting\", \"corruption\",\n",
        "    \"corrupts\", \"corruptted\", \"costlier\", \"costly\", \"counter-productive\",\n",
        "    \"counterproductive\", \"coupists\", \"covetous\", \"coward\", \"cowardly\", \"crabby\",\n",
        "    \"crack\", \"cracked\", \"cracks\", \"craftily\", \"craftly\", \"crafty\", \"cramp\", \"cramped\",\n",
        "    \"cramping\", \"cranky\", \"crap\", \"crappy\", \"craps\", \"crash\", \"crashed\", \"crashes\",\n",
        "    \"crashing\", \"crass\", \"craven\", \"cravenly\", \"craze\", \"crazily\", \"craziness\",\n",
        "    \"crazy\", \"creak\", \"creaking\", \"creaks\", \"credulous\", \"creep\", \"creeping\",\n",
        "    \"creeps\", \"creepy\", \"crept\", \"crime\", \"criminal\", \"cringe\", \"cringed\",\n",
        "    \"cringes\", \"cripple\", \"crippled\", \"cripples\", \"crippling\", \"crisis\", \"critic\",\n",
        "    \"critical\", \"criticism\", \"criticisms\", \"criticize\", \"criticized\", \"criticizing\",\n",
        "    \"critics\", \"cronyism\", \"crook\", \"crooked\", \"crooks\", \"crowded\", \"crowdedness\",\n",
        "    \"crude\", \"cruel\", \"crueler\", \"cruelest\", \"cruelly\", \"cruelness\", \"cruelties\",\n",
        "    \"cruelty\", \"crumble\", \"crumbling\", \"crummy\", \"crumple\", \"crumpled\", \"crumples\",\n",
        "    \"crush\", \"crushed\", \"crushing\", \"cry\", \"culpable\", \"culprit\", \"cumbersome\",\n",
        "    \"cunt\", \"cunts\", \"cuplrit\", \"curse\", \"cursed\", \"curses\", \"curt\", \"cuss\",\n",
        "    \"cussed\", \"cutthroat\", \"cynical\", \"cynicism\", \"d*mn\", \"damage\", \"damaged\",\n",
        "    \"damages\", \"damaging\", \"damn\", \"damnable\", \"damnably\", \"damnation\", \"damned\",\n",
        "    \"damning\", \"damper\", \"danger\", \"dangerous\", \"dangerousness\", \"dark\", \"darken\",\n",
        "    \"darkened\", \"darker\", \"darkness\", \"dastard\", \"dastardly\", \"daunt\", \"daunting\",\n",
        "    \"dauntingly\", \"dawdle\", \"daze\", \"dazed\", \"dead\", \"deadbeat\", \"deadlock\", \"deadly\",\n",
        "    \"deadweight\", \"deaf\", \"dearth\", \"death\", \"debacle\", \"debase\", \"debasement\",\n",
        "    \"debaser\", \"debatable\", \"debauch\", \"debaucher\", \"debauchery\", \"debilitate\",\n",
        "    \"debilitating\", \"debility\", \"debt\", \"debts\", \"decadence\", \"decadent\", \"decay\",\n",
        "    \"decayed\", \"deceit\", \"deceitful\", \"deceitfully\", \"deceitfulness\", \"deceive\",\n",
        "    \"deceiver\", \"deceivers\", \"deceiving\", \"deception\", \"deceptive\", \"deceptively\",\n",
        "    \"declaim\", \"decline\", \"declines\", \"declining\", \"decrement\", \"decrepit\",\n",
        "    \"decrepitude\", \"decry\", \"defamation\", \"defamations\", \"defamatory\", \"defame\",\n",
        "    \"defect\", \"defective\", \"defects\", \"defensive\", \"defiance\", \"defiant\",\n",
        "    \"defiantly\", \"deficiencies\", \"deficiency\", \"deficient\", \"defile\", \"defiler\",\n",
        "    \"deform\", \"deformed\", \"defrauding\", \"defunct\", \"defy\", \"degenerate\",\n",
        "    \"degenerately\", \"degeneration\", \"degradation\", \"degrade\", \"degrading\",\n",
        "    \"degradingly\", \"dehumanization\", \"dehumanize\", \"deign\", \"deject\", \"dejected\",\n",
        "    \"dejectedly\", \"dejection\", \"delay\", \"delayed\", \"delaying\", \"delays\",\n",
        "    \"delinquency\", \"delinquent\", \"delirious\", \"delirium\", \"delude\", \"deluded\",\n",
        "    \"deluge\", \"delusion\", \"delusional\", \"delusions\", \"demean\", \"demeaning\",\n",
        "    \"demise\", \"demolish\", \"demolisher\", \"demon\", \"demonic\", \"demonize\",\n",
        "    \"demonized\", \"demonizes\", \"demonizing\", \"demoralize\", \"demoralizing\",\n",
        "    \"demoralizingly\", \"denial\", \"denied\", \"denies\", \"denigrate\", \"denounce\",\n",
        "    \"dense\", \"dent\", \"dented\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "ht_5G6C3Jox8"
      },
      "id": "ht_5G6C3Jox8",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import spacy\n",
        "from spacy.tokens import Token\n",
        "from negspacy.negation import Negex\n",
        "from negspacy.termsets import termset\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Set up the termset and get patterns\n",
        "ts = termset(\"en\")\n",
        "\n",
        "# Register the 'negex' extension on tokens (force=True to avoid extension errors)\n",
        "Token.set_extension(\"negex\", default=False, force=True)\n",
        "\n",
        "# Add Negex component with configuration using termset patterns\n",
        "nlp.add_pipe(\"negex\", last=True, config={\n",
        "    \"neg_termset\": ts.get_patterns(),\n",
        "    \"ent_types\": [\"PERSON\", \"NOUN\"],  # Relevant entity types for negation analysis\n",
        "    \"chunk_prefix\": [\"no\", \"not\", \"never\", \"without\"]  # Define common negation indicators\n",
        "})\n",
        "\n",
        "# Define a list of negative words\n",
        "#negative_words = [\"bad\", \"worse\", \"worst\", \"terrible\", \"awful\", \"hate\", \"dislike\", \"not good\", \"no\", \"never\"]\n",
        "\n",
        "# Load training data\n",
        "trainData = pd.read_csv('/content/drive/MyDrive/finalDataset.csv')\n",
        "trainData = trainData.dropna()\n",
        "print(trainData.head())\n",
        "\n",
        "# Clean tweets\n",
        "clean_tweets = trainData['cleaned_text']\n",
        "print(clean_tweets)\n",
        "\n",
        "# Analyze negation with Negex and negative words\n",
        "def analyze_negation(text):\n",
        "    doc = nlp(text)\n",
        "    analyzed_tokens = []\n",
        "\n",
        "    for token in doc:\n",
        "        # Check if the token is a negative word\n",
        "        if token.text.lower() in negative_words:\n",
        "            analyzed_tokens.append(token.text + \"NEG\")  # Mark as negative word\n",
        "        elif token._.negex:\n",
        "            analyzed_tokens.append(token.text + \"NEG\")  # Mark as negated\n",
        "        else:\n",
        "            analyzed_tokens.append(token.text)  # Keep the token as is\n",
        "\n",
        "    return \" \".join(analyzed_tokens)\n",
        "\n",
        "# Apply Negex analysis to cleaned tweets\n",
        "clean_tweets = clean_tweets.apply(analyze_negation)\n",
        "\n",
        "# Vectorize tweets\n",
        "vectorizer = TfidfVectorizer(max_features=None, ngram_range=(1, 2))\n",
        "train_data_features = vectorizer.fit_transform(clean_tweets)\n",
        "print(train_data_features.shape)\n",
        "\n",
        "# Get labels\n",
        "y_train = trainData['sentiment']\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
        "model.fit(train_data_features, y_train)\n",
        "\n",
        "# Load test data\n",
        "testData = pd.read_csv('/content/drive/MyDrive/finalDatasetTest.csv')\n",
        "testData = testData.dropna()\n",
        "print(testData.head())\n",
        "\n",
        "# Clean test tweets\n",
        "clean_tweets_test = testData['cleaned_text']\n",
        "print(clean_tweets_test)\n",
        "\n",
        "# Analyze negation on test data\n",
        "clean_tweets_test = clean_tweets_test.apply(analyze_negation)\n",
        "\n",
        "# Get true labels\n",
        "y_test = testData['sentiment']\n",
        "\n",
        "# Vectorize test data\n",
        "test_data_features = vectorizer.transform(clean_tweets_test)\n",
        "result = model.predict(test_data_features)\n",
        "\n",
        "# Create output DataFrame\n",
        "output = pd.DataFrame(data={\"id\": testData[\"textID\"], \"sentiment\": result})\n",
        "\n",
        "# Save predictions to CSV\n",
        "output.to_csv(\"resultPredictionsOnTest.csv\", index=False, quoting=3)\n",
        "\n",
        "# Calculate and print metrics\n",
        "accuracy = accuracy_score(y_test, result)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "precision = precision_score(y_test, result, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, result, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, result, average='weighted', zero_division=0)\n",
        "\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "# Print a classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, result))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_KLX8BuPG3J",
        "outputId": "375dfdc7-fce5-40fc-ec68-82d8e61bc595"
      },
      "id": "2_KLX8BuPG3J",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       textID                                               text  \\\n",
            "0  cb774db0d1                I`d have responded, if I were going   \n",
            "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
            "2  088c60f138                          my boss is bullying me...   \n",
            "3  9642c003ef                     what interview! leave me alone   \n",
            "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
            "\n",
            "                         selected_text sentiment Time of Tweet Age of User  \\\n",
            "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
            "1                             Sooo SAD  negative          noon       21-30   \n",
            "2                          bullying me  negative         night       31-45   \n",
            "3                       leave me alone  negative       morning       46-60   \n",
            "4                        Sons of ****,  negative          noon       60-70   \n",
            "\n",
            "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \\\n",
            "0  Afghanistan          38928346         652860.0               60   \n",
            "1      Albania           2877797          27400.0              105   \n",
            "2      Algeria          43851044        2381740.0               18   \n",
            "3      Andorra             77265            470.0              164   \n",
            "4       Angola          32866272        1246700.0               26   \n",
            "\n",
            "                           cleaned_text  \\\n",
            "0                       responded going   \n",
            "1               soon sad miss san diego   \n",
            "2                          boy building   \n",
            "3                 interview leave alone   \n",
            "4  son could put release already bought   \n",
            "\n",
            "                                              tokens  \n",
            "0                             ['responded', 'going']  \n",
            "1            ['sooo', 'sad', 'miss', 'san', 'diego']  \n",
            "2                                ['bos', 'bullying']  \n",
            "3                    ['interview', 'leave', 'alone']  \n",
            "4  ['son', 'could', 'put', 'release', 'already', ...  \n",
            "0                                          responded going\n",
            "1                                  soon sad miss san diego\n",
            "2                                             boy building\n",
            "3                                    interview leave alone\n",
            "4                     son could put release already bought\n",
            "                               ...                        \n",
            "27475    wish could come see denver husband lost job af...\n",
            "27476    wondered rake client made clear net force des ...\n",
            "27477    may good enjoy break probably need hectic week...\n",
            "27478                                                worth\n",
            "27479                      flirting going at smile may hug\n",
            "Name: cleaned_text, Length: 27368, dtype: object\n",
            "(27368, 132138)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       textID                                               text sentiment  \\\n",
            "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
            "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive   \n",
            "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative   \n",
            "3  01082688c6                                        happy bday!  positive   \n",
            "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive   \n",
            "\n",
            "  Time of Tweet Age of User      Country  Population -2020  Land Area (Km²)  \\\n",
            "0       morning        0-20  Afghanistan        38928346.0         652860.0   \n",
            "1          noon       21-30      Albania         2877797.0          27400.0   \n",
            "2         night       31-45      Algeria        43851044.0        2381740.0   \n",
            "3       morning       46-60      Andorra           77265.0            470.0   \n",
            "4          noon       60-70       Angola        32866272.0        1246700.0   \n",
            "\n",
            "   Density (P/Km²)                                       cleaned_text  \\\n",
            "0             60.0                                   last session day   \n",
            "1            105.0  shanghai also really exciting precisely skyscr...   \n",
            "2             18.0  recession hit veronique branquinho quit compan...   \n",
            "3            164.0                                     happy birthday   \n",
            "4             26.0                                               like   \n",
            "\n",
            "                                              tokens  \n",
            "0                         ['last', 'session', 'day']  \n",
            "1  ['shanghai', 'also', 'really', 'exciting', 'pr...  \n",
            "2  ['recession', 'hit', 'veronique', 'branquinho'...  \n",
            "3                              ['happy', 'birthday']  \n",
            "4                                           ['like']  \n",
            "0                                        last session day\n",
            "1       shanghai also really exciting precisely skyscr...\n",
            "2       recession hit veronique branquinho quit compan...\n",
            "3                                          happy birthday\n",
            "4                                                    like\n",
            "                              ...                        \n",
            "3529                                      tired sleep try\n",
            "3530    alone old house thanks net keep alive kicking ...\n",
            "3531    know mean little dog sinking depression want m...\n",
            "3532            supra next couture video going love video\n",
            "3533                                    omgssh and cut by\n",
            "Name: cleaned_text, Length: 3517, dtype: object\n",
            "Accuracy: 0.6943\n",
            "Precision: 0.6993\n",
            "Recall: 0.6943\n",
            "F1 Score: 0.6945\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.61      0.66       999\n",
            "     neutral       0.64      0.72      0.68      1415\n",
            "    positive       0.76      0.73      0.75      1103\n",
            "\n",
            "    accuracy                           0.69      3517\n",
            "   macro avg       0.71      0.69      0.70      3517\n",
            "weighted avg       0.70      0.69      0.69      3517\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}